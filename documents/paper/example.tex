\section{High energy physics applications}
\label{sec:hep}

\subsection{B physics}

\subsubsection{Data sample}

In order to demonstrate the functionality of the toolkit, data samples generated
from the RapidSim package~\cite{rapid} are used. The interesting signal is chosen
to be the $\Bs\to\jpsi(\to\mu\mu )\phi (\to\PK\PK )$ decay, and the background is
the $\Dz\to\pi\pi\pi\pi$ decay. A total of 10000 candidates is generated for each decay.

\subsubsection{Training of the original classifier}
\label{sec:orig_training}

%The machine learning classifier chosen is the Multi-layer perceptron of SciKit-Learn,
%which is constructed as
The machine learning classifier chosen is a Keras-based convolutional neural net,
which is constructed as
\begin{lstlisting}
classifier = Sequential()
classifier.add(LocallyConnected1D(
filters = 90, kernel_size = 2,
activation = 'sigmoid',
input_shape = (len(setTrain[0]), 1)))
classifier.add(GlobalMaxPooling1D())
classifier.add(Dense(30, activation = 'sigmoid'))
classifier.add(Dense(1, activation = 'sigmoid'))
classifier.compile(optimizer = 'adam',
loss = 'binary_crossentropy'
, metrics = ['accuracy'])
\end{lstlisting}
%\begin{lstlisting}
%classifier = MLPClassifier(activation='relu',
%    batch_size='auto', beta_1=0.9, beta_2=0.999,
%    early_stopping=False, epsilon=1e-08,
%    hidden_layer_sizes=(3, 3),
%    learning_rate='constant',
%    learning_rate_init=0.001, max_iter=200,
%    nesterovs_momentum=True, power_t=0.5,
%    random_state=1, shuffle=True,
%    solver='lbfgs', tol=0.0001, momentum=0.9,
%    validation_fraction=0.1, verbose=False,
%    warm_start=False, alpha=1e-05)
%\end{lstlisting}

The neural network is trained using kinematic properties of the respective decays.
These include the pseudorapidity, $\eta$, and momentum transverse to the direction of the
input proton beams, \pt, of the decaying particle. In addition, the minimum and maximum \pt and $\eta$
of the final state particles is used. The signal and background distributions of the input variables
are shown in Fig.~\ref{fig:inputs}.
%
\begin{figure*}[t]
\centering
\includegraphics[width=0.33\textwidth]{pt_comp}
\includegraphics[width=0.33\textwidth]{eta_comp}
\includegraphics[width=0.33\textwidth]{minpt_comp}
\includegraphics[width=0.33\textwidth]{maxpt_comp}
\includegraphics[width=0.33\textwidth]{mineta_comp}
\includegraphics[width=0.33\textwidth]{maxeta_comp}
\caption{\small Comparison of the signal and background distributions
used to train the Keras B decay classifier.}
\label{fig:inputs}
\end{figure*}

In the training of the original classifier, half of the data is
reserved in order to test for overtraining.
%The output probability distributions of the signal and background samples
%after the training are shown in Fig.~\ref{fig:output}.
%It can be seen that the test and training samples are in good agreement,
%showing that the original SciKit-Learn classifier is not significantly overtrained.
%\begin{figure*}[t]
%\centering
%\includegraphics[width=0.45\textwidth]{event_probability}
%\caption{\small
%Output distributions of the signal and background training and
%test samples from the classifier after training.
%}
%\label{fig:output}
%\end{figure*}


\subsection{Jet separation}
\label{sec:hepGPD}

\subsubsection{Data sample}

A further demonstration is provided demonstrating a classifiers ability to separate different
kinds of jets. The data sample to show this has been generated from Pythia~\cite{Sjostrand:2007gs}, using the FastJet~\cite{Cacciari:2011ma} package
to create the jet objects.
The signal sample is chosen to be the jet produced in association with a W boson.
The background sample is chosen to be jets from Quantum Chromodynamics processes.

\subsubsection{Training of the original classifier}

The machine learning classifier chosen is also a Keras-based convolutional neural net,
constructed in an identical way as described in Sec~\ref{sec:orig_training}.

The neural network is trained using 4 jet properties consisting of the energy,
the combined mass of the particles in the jet, the pseudorapidity, and the jet $K_t$ (the definition
of the $K_t$ variable and a review of jet reconstruction algorithms
may be found in Ref.~\cite{Atkin:2015msa}).
The signal and background distributions of the input variables
are shown in Fig.~\ref{fig:inputsGPD}.
%
\begin{figure*}[t]
\centering
\includegraphics[width=0.33\textwidth]{et_comp_gpd}
\includegraphics[width=0.33\textwidth]{eta_comp_gpd}
\includegraphics[width=0.33\textwidth]{m_comp_gpd}
\includegraphics[width=0.33\textwidth]{kt_comp_gpd}
\caption{\small Comparison of the signal and background distributions
used to train the Keras classifier .}
\label{fig:inputsGPD}
\end{figure*}

\subsection{Drone conversions}

The drone neural networks are trained following the procedure outlined in Sec.~\ref{sec:dlearn},
In total, 1500 epochs are used with
the learning rate of the stochastic gradient descent set to 0.05.
The value of $\kappa$ is chosen to be 0.01.

The loss history of the drone approximations are shown in Fig.~\ref{fig:loss}
as a function of epoch number.
The convergence is also shown in Fig.~\ref{fig:iterdiff}, which shows
the difference in the value of the loss function with respect to the previous
epoch. The epochs that trigger an increase in the number of hyperparameters
are also overlaid.
In total for the case of B decays, an increase was triggered 38 times compared to 10 times for the
jet separation classifier. The total number
of parameters in the final drone neural networks are therefore 302 and 106 for the B decay drone
and the jet separation drone, respectively. It is interesting
to note that with the algorithm design of Sec.~\ref{sec:dlearn}, the introduction
of the new parameter space causes the drone networks to learn faster, as evidenced by
increases in Fig.~\ref{fig:iterdiff} with continuing descent of the loss functions.
%
The performance of the original classifiers compared to the drone classifiers are shown in Figure~\ref{fig:roc}.
\begin{figure*}[t]
\centering
\includegraphics[width=0.45\textwidth]{loss_history}
\includegraphics[width=0.45\textwidth]{loss_history_gpd}
\caption{\small
Convergence of the loss function during the drone training
  for the case of the B
  decay (left) and jet separation (right) examples.
}
\label{fig:loss}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[width=0.45\textwidth]{diff_history}
\includegraphics[width=0.45\textwidth]{diff_history_gpd}
\caption{\small
Difference in the loss function with respect to the previous iteration
  for the case of the B
  decay (left) and jet separation (right) examples.
  The green triangles
depict the epoch number in which the number of hyperperameters was increased.
}
\label{fig:iterdiff}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[width=0.45\textwidth]{roc}
\includegraphics[width=0.45\textwidth]{roc_gpd}
\caption{\small
  Signal efficiency versus background rejection of the original classifier (red) and drone
  approximation (blue)
  for the case of the B
  decay (left) and jet separation (right) examples.
}
\label{fig:roc}
\end{figure*}


\section{Drone storage and transferability and suitability
for low-latency environments}
\label{sec:storage}

The hyperparameters and structure of the drone are required to be
portable and easily stored for later usage. For this the {\tt JSON} format was chosen as
mediator. It is human-readable and easily accessible in the {\tt Python} and {\tt C++}
environments commonly used in HEP. Thus, it is readily deployable in both personal and production environments.

Provided is a tool to export and save a drone neural network to a {\tt JSON}
formatted file which preserves the input \& output structure,
the layers and nodes, all hyperparameters and activation functions.
The drone configuration is later read in by an equivalent tool into the production software framework,
which then constructs a class object based on the Keras model. The {\tt C++} class implements
a flexible member structure that is capable of completely reproducing the original drone. The production
implementation may be used for all data reduction levels, be it in the form of a low-latency trigger
for example up to the latest stages of data handling and output.

A major advantage of this method is that analysts and users have the full freedom of latest developments
of industry standards, but need only to support a more manageable implementation in the low-latency
software. This is further aided by projects such as ONNX~\cite{ONNX}, which enable classifiers from a wider
range of software packages to be converted to a framework in which an approximation converter
is available.

The identical performance show in Fig.~\ref{fig:roc} is clearly the ideal scenario, even though
such good agreement is not always required to give better results than other low-latency methods.
However it is worth noting that the drones created in the examples of Sec.~\ref{sec:hep} are faster to
evaluate. The comparison of the time taken for each model evaluation, determined from a desktop
using a Intel Core i7-4770 processor is shown in Table~\ref{tab:comp}.
\begin{table}[t]
  \centering
  \caption{Processing time comparisons of the original models and drone
  approximations for the HEP examples. \label{tab:comp}}
  \begin{tabular}{l|rr}
                   & original model                  & drone \\
    \hline
    B decay        & $19.3 \pm 0.2 \times 10^{-5}$ s & $2.7 \pm 0.2 \times 10^{-5}$ s \\
    jet separation & $25.2 \pm 0.2 \times 10^{-5}$ s & $2.9 \pm 0.2 \times 10^{-5}$ s \\
  \end{tabular}
\end{table}
