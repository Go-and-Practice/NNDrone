\section{High energy physics applications}
\label{sec:hep}

\subsection{B physics}

\subsubsection{Data sample}

In order to demonstrate the functionality of the toolkit, data samples generated
from the RapidSim package~\cite{rapid} are used. The interesting signal is chosen
to be the $\Bs\to\jpsi(\to\mu\mu )\phi (\to\PK\PK )$ decay, and the background is
the $\Dz\to\pi\pi\pi\pi$ decay. A total of 10000 candidates is generated for each decay.

\subsubsection{Training of the original classifier}
\label{sec:orig_training}

%The machine learning classifier chosen is the Multi-layer perceptron of SciKit-Learn,
%which is constructed as
The machine learning classifier, using the Keras framework~\cite{keras},
is constructed as
which is constructed as
\begin{lstlisting}
classifier = Sequential()
classifier.add(LocallyConnected1D(
filters = 90, kernel_size = 2,
activation = 'sigmoid',
input_shape = (len(setTrain[0]), 1)))
classifier.add(GlobalMaxPooling1D())
classifier.add(Dense(30, activation = 'sigmoid'))
classifier.add(Dense(1, activation = 'sigmoid'))
classifier.compile(optimizer = 'adam',
loss = 'binary_crossentropy'
, metrics = ['accuracy'])
\end{lstlisting}
%\begin{lstlisting}
%classifier = MLPClassifier(activation='relu',
%    batch_size='auto', beta_1=0.9, beta_2=0.999,
%    early_stopping=False, epsilon=1e-08,
%    hidden_layer_sizes=(3, 3),
%    learning_rate='constant',
%    learning_rate_init=0.001, max_iter=200,
%    nesterovs_momentum=True, power_t=0.5,
%    random_state=1, shuffle=True,
%    solver='lbfgs', tol=0.0001, momentum=0.9,
%    validation_fraction=0.1, verbose=False,
%    warm_start=False, alpha=1e-05)
%\end{lstlisting}

The neural network is trained using kinematic properties of the respective decays.
These include the pseudorapidity, $\eta$, and momentum transverse to the direction of the
input proton beams, \pt, of the decaying particle. In addition, the minimum and maximum \pt and $\eta$
of the final state particles is used. The signal and background distributions of the input variables
are shown in Fig.~\ref{fig:inputs}.
%
\begin{figure*}[t]
\centering
\includegraphics[width=0.33\textwidth]{pt_comp}
\includegraphics[width=0.33\textwidth]{eta_comp}
\includegraphics[width=0.33\textwidth]{minpt_comp}
\includegraphics[width=0.33\textwidth]{maxpt_comp}
\includegraphics[width=0.33\textwidth]{mineta_comp}
\includegraphics[width=0.33\textwidth]{maxeta_comp}
\caption{\small Comparison of the signal and background distributions
used to train the Keras B decay classifier.}
\label{fig:inputs}
\end{figure*}

In the training of the original classifier, half of the data is
reserved in order to test for overtraining.
%The output probability distributions of the signal and background samples
%after the training are shown in Fig.~\ref{fig:output}.
%It can be seen that the test and training samples are in good agreement,
%showing that the original SciKit-Learn classifier is not significantly overtrained.
%\begin{figure*}[t]
%\centering
%\includegraphics[width=0.45\textwidth]{event_probability}
%\caption{\small
%Output distributions of the signal and background training and
%test samples from the classifier after training.
%}
%\label{fig:output}
%\end{figure*}


\subsection{Jet separation}
\label{sec:hepGPD}

\subsubsection{Data sample}

A further demonstration is provided demonstrating a classifiers ability to separate different
kinds of jets. The data sample to show this has been generated from Pythia~\cite{Sjostrand:2007gs}
simulating pp collisions at 14\tev.
The jets themselves are reconstructed in the Rivet analysis framework~\cite{Buckley:2010ar}
and are created using the FastJet~\cite{Cacciari:2011ma} package using the $K_t$ algorithm~\cite{Salam:2007xv}
(the definition
of the $K_t$ variable and a review of jet reconstruction algorithms
may be found in Ref.~\cite{Atkin:2015msa}).
A jet \pt requirement of 20\gev is imposed on all jets.
All other parameters remain at the default values for Rivet version 2.5.4.
The signal sample is chosen to correspond to a $qg\to qg$ type of interaction,
whereas the background is chosen to correspond to a $gg \to gg$ type.
Jets that originate from gluons in the final state form a background to many
analyses, therefore efficient rejection of such processes is important in making
measurements~\cite{Komiske:2016rsd}.

\subsubsection{Training of the original classifier}

The machine learning classifier chosen is also a Keras-based convolutional neural net,
constructed in an similar way as described in Sec~\ref{sec:orig_training}.
\begin{lstlisting}
classifier = Sequential()
classifier.add(LocallyConnected1D(
filters = 32, kernel_size = 2,
activation = 'relu', input_shape = sig_data[0].shape))
classifier.add(MaxPooling1D(pool_size = 3,
strides = 1))
classifier.add(Dropout(0.25))
classifier.add(Conv1D(filters = 32, kernel_size = 3,
activation = 'relu'))
classifier.add(MaxPooling1D(pool_size = 2,
strides = 2))
classifier.add(Dropout(0.25))
classifier.add(Conv1D(filters = 32, kernel_size = 2,
activation = 'relu'))
classifier.add(MaxPooling1D(pool_size = 2,
strides = 2))
classifier.add(Dropout(0.5))
classifier.add(Flatten())
classifier.add(Dense(50, activation = 'relu'))
classifier.add(Dense(1, activation = 'sigmoid'))
classifier.compile(optimizer = 'adam',
loss = 'binary_crossentropy'
, metrics = ['accuracy'])
earlystop = EarlyStopping(patience = 3)
model.fit(setTrain, labels, batch_size = batchSize,
epochs = epochNum, validation_data = (setTest, labels),
callbacks = [earlystop])
\end{lstlisting}

The training data is prepared as described in Sec. 3.1 of~\cite{Komiske:2016rsd}. Four images
in two dimensions (azimuthal angle, $\phi$, and rapidity, $\eta$) are created from the multiplicity and
transverse energy of charged and neutral particles. The resulting images are centred and
cropped in a region $\eta$, $\phi$ $\in (-0.4, 0.4)$. Finally the images are normalised
(pixel intensities sum to 1, $\sum_{ij} I_{ij} = 1$), zero-centred (normalised mean subtracted
from pixel intensity, $I_{ij}^{'} = I_{ij} - \mu_{ij}$) and standardised (divide pixel intensity
by the standard deviation, $I^{''}_{ij} = I^{'}_{ij} / (\sigma_{ij} + r)$, $r = 10^-5$).
Finally, a further step is added where the images are projected in two 1-D projections each and
four channels are considered: charged \& neutral particle transverse energy and charged \& neutral
particle multiplicity. The resulting 1-D feature vector is appended with some
total kinematic properties such as the $K_t$ of the jet, its momentum, energy, mass,
multiplicity, etc. - producing a total of 140 individual features.
% The neural network is trained using 4 jet properties consisting of the energy,
% the combined mass of the particles in the jet, the pseudorapidity, and the jet $K_t$.
The signal and background distributions of the input variables
are shown in Fig.~\ref{fig:inputsGPD}.
%
\begin{figure*}[t]
\centering
\includegraphics[width=0.33\textwidth]{et_comp_gpd}
\includegraphics[width=0.33\textwidth]{eta_comp_gpd}
\includegraphics[width=0.33\textwidth]{m_comp_gpd}
\includegraphics[width=0.33\textwidth]{kt_comp_gpd}
\caption{\small Comparison of the signal and background distributions
used to train the Keras classifier .}
\label{fig:inputsGPD}
\end{figure*}

\subsection{Drone conversions}

The drone neural networks are trained following the procedure outlined in Sec.~\ref{sec:dlearn},
In total, 1500 epochs are used with
the learning rate of the stochastic gradient descent set to 0.05.
The value of $\kappa$ is chosen to be 0.01.

The loss history of the drone approximations are shown in Fig.~\ref{fig:loss}
as a function of epoch number.
The convergence is also shown in Fig.~\ref{fig:iterdiff}, which shows
the difference in the value of the loss function with respect to the previous
epoch. The epochs that trigger an increase in the number of hyperparameters
are also overlaid.
In total for the case of B decays, an increase was triggered 38 times compared to 67 times for the
jet separation classifier. The total number
of parameters in the final drone neural networks are therefore 302 and 10152 for the B decay drone
and the jet separation drone, respectively. It is interesting
to note that with the algorithm design of Sec.~\ref{sec:dlearn}, the introduction
of the new parameter space causes the drone networks to learn faster, as evidenced by
increases in Fig.~\ref{fig:iterdiff} with continuing descent of the loss functions.
%
The performance of the original classifiers compared to the drone classifiers are shown in Figure~\ref{fig:roc}.
\begin{figure*}[t]
\centering
\includegraphics[width=0.45\textwidth]{loss_history}
\includegraphics[width=0.45\textwidth]{loss_history_gpd}
\caption{\small
Convergence of the loss function during the drone training
  for the case of the B
  decay (left) and jet separation (right) examples.
}
\label{fig:loss}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[width=0.45\textwidth]{diff_history}
\includegraphics[width=0.45\textwidth]{diff_history_gpd}
\caption{\small
Difference in the loss function with respect to the previous iteration
  for the case of the B
  decay (left) and jet separation (right) examples.
  The green triangles
depict the epoch number in which the number of hyperperameters was increased.
}
\label{fig:iterdiff}
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[width=0.45\textwidth]{roc}
\includegraphics[width=0.45\textwidth]{roc_gpd}
\caption{\small
  Signal efficiency versus background rejection of the original classifier (red) and drone
  approximation (blue)
  for the case of the B
  decay (left) and jet separation (right) examples.
}
\label{fig:roc}
\end{figure*}


\section{Drone storage and transferability and suitability
for low-latency environments}
\label{sec:storage}

The hyperparameters and structure of the drone are required to be
portable and easily stored for later usage. For this the {\tt JSON} format was chosen as
mediator. It is human-readable and easily accessible in the {\tt Python} and {\tt C++}
environments commonly used in HEP. Thus, it is readily deployable in both personal and production environments.

Provided is a tool to export and save a drone neural network to a {\tt JSON}
formatted file which preserves the input \& output structure,
the layers and nodes, all hyperparameters and activation functions.
The drone configuration is later read in by an equivalent tool into the production software framework,
which then constructs a class object based on the Keras model. The {\tt C++} class implements
a flexible member structure that is capable of completely reproducing the original drone. The production
implementation may be used for all data reduction levels, be it in the form of a low-latency trigger
for example up to the latest stages of data handling and output.

A major advantage of this method is that analysts and users have the full freedom of latest developments
of industry standards, but need only to support a more manageable implementation in the low-latency
software. This is further aided by projects such as ONNX~\cite{ONNX}, which enable classifiers from a wider
range of software packages to be converted to a framework in which an approximation converter
is available.

The identical performance show in Fig.~\ref{fig:roc} is clearly the ideal scenario, even though
such good agreement is not always required to give better results than other low-latency methods.
However it is worth noting that the drones created in the examples of Sec.~\ref{sec:hep} are faster to
evaluate. The comparison of the time taken for each model evaluation, determined from a desktop
using a Intel Core i7-4770 processor is shown in Table~\ref{tab:comp}.
\begin{table}[t]
  \centering
  \caption{Hyperparameter number comparisons of the original models and drone
  approximations for the HEP examples. \label{tab:comp_param}}
  \begin{tabular}{l|rr}
                   & original model                  & drone \\
    \hline
    B decay        & 4,111 & 302 \\
    jet separation & 71,429 & 10152 \\
  \end{tabular}
\end{table}

\begin{table}[t]
  \centering
  \caption{Processing time comparisons of the original models and drone
  approximations for the HEP examples. \label{tab:comp}}
  \begin{tabular}{l|rr}
                   & original model                  & drone \\
    \hline
    B decay        & $19.3 \pm 0.2 \times 10^{-5}$ s & $2.7 \pm 0.2 \times 10^{-5}$ s \\
    jet separation &  $3.2 \pm 0.2 \times 10^{-3}$ s & $4.3 \pm 0.2 \times 10^{-4}$ s \\
  \end{tabular}
\end{table}
